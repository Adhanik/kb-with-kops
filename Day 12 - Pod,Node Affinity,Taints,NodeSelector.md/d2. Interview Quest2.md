


## **1. Node Affinity** (Pod → Node Relationship)  

### **Scenario:**  
Schedule a pod only on nodes labeled with `disktype=ssd`.  

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: ssd-pod
spec:
  containers:
  - name: busybox
    image: busybox
  affinity:
    nodeAffinity:
      requiredDuringSchedulingIgnoredDuringExecution:  # Hard requirement
        nodeSelectorTerms:
        - matchExpressions:
          - key: disktype
            operator: In
            values:
            - ssd
```

**Explanation:**  
- The pod **must** be scheduled on nodes labeled `disktype=ssd`.  
- `requiredDuringSchedulingIgnoredDuringExecution`: **Hard rule**—if no matching node exists, the pod won't be scheduled.  

---

## **2. Pod Affinity** (Pod → Pod Relationship)  

### **Scenario:**  
Ensure a web app pod is scheduled **near** its backend service pod for low-latency communication.  

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: web-app-pod
  labels:
    app: web
spec:
  containers:
  - name: nginx
    image: nginx
  affinity:
    podAffinity:
      requiredDuringSchedulingIgnoredDuringExecution:  # Hard requirement
      - labelSelector:
          matchLabels:
            app: backend
        topologyKey: "kubernetes.io/hostname"  # Schedule in the same node
```

**Explanation:**  
- Pods with label `app=backend` must exist on the same node (`topologyKey: hostname`).  
- Used when latency-sensitive microservices need proximity.  

---

## **3. Pod Anti-Affinity** (Pod → Pod Relationship)  

### **Scenario:**  
Ensure two replicas of the same app are scheduled **on different nodes** for fault tolerance.  

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: app-replica
  labels:
    app: my-app
spec:
  containers:
  - name: nginx
    image: nginx
  affinity:
    podAntiAffinity:
      requiredDuringSchedulingIgnoredDuringExecution:  # Hard rule
      - labelSelector:
          matchLabels:
            app: my-app
        topologyKey: "kubernetes.io/hostname"  # Spread pods across nodes
```

**Explanation:**  
- Ensures pods with label `app=my-app` are **not** scheduled on the same node.  
- Provides **high availability** by spreading replicas across nodes.  

---

## **4. Taints and Tolerations** (Node → Pod Relationship)  

### **Scenario 1:**  
Reserve a node only for logging pods.  

1. **Taint the node:**
```bash
kubectl taint nodes node1 dedicated=logging:NoSchedule
```

2. **Pod configuration with toleration:**  
```yaml
apiVersion: v1
kind: Pod
metadata:
  name: logging-pod
spec:
  tolerations:
  - key: "dedicated"
    operator: "Equal"
    value: "logging"
    effect: "NoSchedule"  # Allows scheduling on tainted nodes
  containers:
  - name: fluentd
    image: fluentd
```

**Explanation:**  
- Node is tainted (`dedicated=logging`), blocking all pods except those with a **toleration**.  
- Only pods with the toleration (like this logging pod) can run on the node.  

---

### **Scenario 2:**  
Taint a node for NoExecute (force pod eviction).  

1. **Taint the node:**
```bash
kubectl taint nodes node2 critical=high:NoExecute
```

2. **Pod toleration to avoid eviction:**
```yaml
apiVersion: v1
kind: Pod
metadata:
  name: critical-pod
spec:
  tolerations:
  - key: "critical"
    operator: "Equal"
    value: "high"
    effect: "NoExecute"  # Prevents eviction
  containers:
  - name: redis
    image: redis
```

**Explanation:**  
- `NoExecute` evicts pods that **don't** tolerate the taint.  
- Pods like this example won't be evicted due to the toleration.  

---

## **5. Combined Example: Node Affinity with Taints**

### **Scenario:**  
Schedule GPU workloads only on GPU nodes while tolerating taints.  

1. **Taint the GPU nodes:**
```bash
kubectl taint nodes gpu-node gpu=true:NoSchedule
```

2. **Pod configuration:**
```yaml
apiVersion: v1
kind: Pod
metadata:
  name: gpu-pod
spec:
  tolerations:
  - key: "gpu"
    operator: "Equal"
    value: "true"
    effect: "NoSchedule"
  affinity:
    nodeAffinity:
      requiredDuringSchedulingIgnoredDuringExecution:
        nodeSelectorTerms:
        - matchExpressions:
          - key: hardware
            operator: In
            values:
            - gpu
  containers:
  - name: tensorflow
    image: tensorflow/tensorflow:latest-gpu
```

**Explanation:**  
- Uses **Node Affinity** to schedule pods on GPU-labeled nodes.  
- Uses **Tolerations** to bypass the GPU node's taint (`gpu=true:NoSchedule`).  

---

## **6. Bonus Questions and YAML Examples**

1. **Q: How do you schedule a pod to prefer nodes in a specific zone (soft rule)?**  
```yaml
affinity:
  nodeAffinity:
    preferredDuringSchedulingIgnoredDuringExecution:
    - weight: 1
      preference:
        matchExpressions:
        - key: topology.kubernetes.io/zone
          operator: In
          values:
          - us-west-1a
```

**Answer**: Use **preferredDuringSchedulingIgnoredDuringExecution** for soft rules where pods should **prefer** but are **not restricted** to nodes in a specific zone.  

---

2. **Q: How to spread pods evenly across zones?**  
```yaml
topologySpreadConstraints:
- maxSkew: 1
  topologyKey: "topology.kubernetes.io/zone"
  whenUnsatisfiable: DoNotSchedule
  labelSelector:
    matchLabels:
      app: backend
```

**Answer**: Use **topologySpreadConstraints** to ensure balanced distribution across zones for resilience.  

---

### **Key Takeaways for Interviews:**
1. **Memorize Concepts with Analogies** (magnets, friends, enemies, bouncers).  
2. **Explain Use Cases Clearly** (low latency, HA, isolation).  
3. **Demonstrate YAML Confidence** (write small snippets quickly).  
4. **Highlight Combined Rules** (Affinity + Tolerations for complex scenarios).  

Let me know if you'd like additional examples!