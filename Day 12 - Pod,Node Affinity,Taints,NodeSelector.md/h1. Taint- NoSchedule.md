

As of now, we dont have any pods deployed on our KB cluster

```
 kubectl get pods
No resources found in default namespace.
ubuntu@ip-172-31-30-169:~$ 

```

We will create a random deployment, before making use of taint and tolerations.

```
kubectl create deployment amit --image=nginx --replicas=3
deployment.apps/amit created

```

## Why no pods are scheduled in Master node?

```
 kubectl get nodes -o wide --no-headers
i-0ae49c553dd44dbe6   Ready   node            20m   v1.28.15   172.20.188.184   <none>   Ubuntu 22.04.5 LTS   6.8.0-1019-aws   containerd://1.7.16
i-0dfe35f99eae2ed49   Ready   control-plane   23m   v1.28.15   172.20.169.243   <none>   Ubuntu 22.04.5 LTS   6.8.0-1019-aws   containerd://1.7.16
i-0e8a5897b99e4741d   Ready   node            20m   v1.28.15   172.20.153.63    <none>   Ubuntu 22.04.5 LTS   6.8.0-1019-aws   containerd://1.7.16
i-0f302835d89b26b7d   Ready   node            20m   v1.28.15   172.20.218.130   <none>   Ubuntu 22.04.5 LTS   6.8.0-1019-aws   containerd://1.7.16
```

If we do describe on control plane node, we can see that by default, this node is tainted with NoSchedule, which means no pods can be mounted on this node.

`kubectl describe nodes i-0dfe35f99eae2ed49` or `kubectl describe nodes i-0dfe35f99eae2ed49 | grep -i NoSchedule`

```
Taints:             node-role.kubernetes.io/control-plane:NoSchedule
```

While for other nodes, we can see that there is no taint on those nodes.

`kubectl describe node i-0e8a5897b99e4741d`

`Taints:             <none>`

## Taint the nodes

Next we will taint the nodes 

`kubectl taint nodes i-0ae49c553dd44dbe6 high-cpu=yes:NoSchedule`
`kubectl taint nodes i-0e8a5897b99e4741d mid-cpu=yes:NoSchedule`
`kubectl taint nodes i-0f302835d89b26b7d low-cpu=yes:NoSchedule`

```
ubuntu@ip-172-31-30-169:~$ kubectl taint nodes i-0f302835d89b26b7d low-cpu=yes:NoSchedule
node/i-0f302835d89b26b7d tainted

```
Once we have tainted all the worker nodes with NoSchedule, we can see that it does not affect the previous deployment that we had done, the pods keep running. However, for the future deployment, since we have tainted now all nodes with NoSchedule, no new pods will be deployed on these nodes.
```
ubuntu@ip-172-31-30-169:~$ kubectl get pods
NAME                   READY   STATUS    RESTARTS   AGE
amit-bf466764d-2lvn2   1/1     Running   0          16m
amit-bf466764d-xnp94   1/1     Running   0          16m
amit-bf466764d-z8m79   1/1     Running   0          16m
ubuntu@ip-172-31-30-169:~$ 

```

Lets see if we can run a pod on any of these nodes now

`kubectl run pod pod1 --image:nginx:latest`

```
kubectl run pod pod1 --image=nginx:latest
pod/pod created

```
We can see that pod remains in pending state
```
kubectl get pods -o wide
NAME                   READY   STATUS    RESTARTS   AGE   IP                NODE                  NOMINATED NODE   READINESS GATES
amit-bf466764d-2lvn2   1/1     Running   0          22m   100.100.104.2     i-0f302835d89b26b7d   <none>           <none>
amit-bf466764d-xnp94   1/1     Running   0          22m   100.126.127.3     i-0e8a5897b99e4741d   <none>           <none>
amit-bf466764d-z8m79   1/1     Running   0          22m   100.105.106.132   i-0ae49c553dd44dbe6   <none>           <none>
pod                    0/1     Pending   0          7s    <none>            <none>                <none>           <none>
```

If we do describe on above pod, we can see that it gives below error. it tells that all 3 nodes have diff taints with NoSchedule.

```
Events:
  Type     Reason            Age   From               Message
  ----     ------            ----  ----               -------
  Warning  FailedScheduling  69s   default-scheduler  0/4 nodes are available: 1 node(s) had untolerated taint {high-cpu: yes}, 1 node(s) had untolerated taint {low-cpu: yes}, 1 node(s) had untolerated taint {mid-cpu: yes}, 1 node(s) had untolerated taint {node-role.kubernetes.io/control-plane: }. preemption: 0/4 nodes are available: 4 Preemption is not helpful for scheduling..
ubuntu@ip-172-31-30-169:~$ 

```

## Where this can be used?

Suppose you have deployed a stateful application on a high performance node, and now you dont want any pods to be deployed further on that node. You can taint that node with NoSchedule, and any future deployments will have no pods deployed on that particular node.

