Hereâ€™s your content converted to Markdown format:

```markdown
# Manual Installation

## Complete Lifecycle of Installing Prometheus Using an Operator

We will go through the entire process step by step, from installing the Prometheus Operator to deploying Prometheus and scraping Node Exporter metrics.

---

## 1. Install the Prometheus Operator (CRDs + Controller)

Prometheus Operator simplifies managing Prometheus by introducing CRDs (Custom Resource Definitions) that define Prometheus instances.

### Step 1.1: Install Prometheus Operator using kubectl

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: prometheus-operator
  namespace: monitoring
  labels:
    app: prometheus-operator
spec:
  replicas: 1
  selector:
    matchLabels:
      app: prometheus-operator
  template:
    metadata:
      labels:
        app: prometheus-operator
    spec:
      containers:
      - name: prometheus-operator
        image: quay.io/prometheus-operator/prometheus-operator:v0.69.1  # Latest version
        args:
          - --kubelet-service=kube-system/kubelet
        ports:
        - name: http
          containerPort: 8080
```

ðŸ“Œ **Explanation:**

- Prometheus Operator is deployed as a **Deployment** (not a DaemonSet).
- It watches for Prometheus CRDs and creates/updates Prometheus instances automatically.
- It exposes an HTTP API on port 8080.

---

## 2. Create CRDs for Prometheus

The Prometheus Operator works by introducing new resource types (CRDs) in Kubernetes:

| **CRD**       | **Purpose**                       |
|---------------|-----------------------------------|
| **Prometheus** | Defines a Prometheus instance     |
| **ServiceMonitor** | Defines services to scrape |
| **PodMonitor** | Defines pods to scrape           |
| **Alertmanager** | Defines an Alertmanager instance |

You donâ€™t need to create these manually if using Helm (which installs them automatically), but if doing it manually, apply this:

```bash
kubectl apply -f https://github.com/prometheus-operator/prometheus-operator/releases/latest/download/bundle.yaml
```

ðŸ“Œ **This installs all CRDs for Prometheus Operator.**

---

## 3. Deploy Prometheus Using the Operator

Now that the Prometheus Operator is running, we create a Prometheus instance using a Prometheus CR.

### Step 3.1: Deploy Prometheus Instance

```yaml
apiVersion: monitoring.coreos.com/v1
kind: Prometheus
metadata:
  name: prometheus
  namespace: monitoring
spec:
  replicas: 2  # Run two instances for redundancy
  serviceAccountName: prometheus
  serviceMonitorSelector: {}  # Selects all ServiceMonitors
  resources:
    requests:
      memory: 400Mi
  storage:
    volumeClaimTemplate:
      spec:
        accessModes: ["ReadWriteOnce"]
        resources:
          requests:
            storage: 5Gi  # Persistent storage for metrics
```

ðŸ“Œ **Explanation:**

- Creates a **Prometheus StatefulSet**.
- `replicas: 2` â†’ Runs two Prometheus instances for high availability.
- Uses a PersistentVolume (5Gi) to store metrics.
- Automatically discovers ServiceMonitors to scrape.

---

## 4. Deploy a Service to Expose Prometheus

To expose Prometheus inside the cluster, we create a Service:

```yaml
apiVersion: v1
kind: Service
metadata:
  name: prometheus
  namespace: monitoring
spec:
  selector:
    prometheus: prometheus
  ports:
  - name: web
    port: 9090
    targetPort: 9090
```

ðŸ“Œ **This allows access to Prometheus UI at** `http://<cluster-ip>:9090`.

---

## 5. Deploy Node Exporter as a DaemonSet

Since Prometheus doesnâ€™t collect node-level metrics by default, we deploy Node Exporter as a DaemonSet.

```yaml
apiVersion: apps/v1
kind: DaemonSet
metadata:
  name: node-exporter
  namespace: monitoring
spec:
  selector:
    matchLabels:
      app: node-exporter
  template:
    metadata:
      labels:
        app: node-exporter
    spec:
      containers:
      - name: node-exporter
        image: prom/node-exporter:v1.6.0
        ports:
        - containerPort: 9100  # Metrics exposed on port 9100
```

ðŸ“Œ **Now, Node Exporter is running on all nodes, exposing metrics at** `http://<node-ip>:9100/metrics`.

---

## 6. Create a ServiceMonitor to Tell Prometheus to Scrape Node Exporter

A **ServiceMonitor** tells Prometheus which services to scrape.

```yaml
apiVersion: monitoring.coreos.com/v1
kind: ServiceMonitor
metadata:
  name: node-exporter-monitor
  namespace: monitoring
spec:
  selector:
    matchLabels:
      app: node-exporter
  endpoints:
  - port: web
    interval: 15s  # Scrape every 15 seconds
```

ðŸ“Œ **Now, Prometheus will scrape Node Exporter running on every node.**

---

## 7. Deploy Grafana for Visualization (Optional)

To visualize the collected metrics, we deploy **Grafana**.

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: grafana
  namespace: monitoring
spec:
  replicas: 1
  selector:
    matchLabels:
      app: grafana
  template:
    metadata:
      labels:
        app: grafana
    spec:
      containers:
      - name: grafana
        image: grafana/grafana:latest
        ports:
        - containerPort: 3000
```

ðŸ“Œ **Access Grafana at** `http://<cluster-ip>:3000` and add Prometheus as a data source.

---

## Final Summary

âœ… **Step 1**: Install Prometheus Operator (Deployment)  
âœ… **Step 2**: Install Prometheus CRDs (Custom Resource Definitions)  
âœ… **Step 3**: Deploy Prometheus (StatefulSet managed by Operator)  
âœ… **Step 4**: Expose Prometheus via a Service  
âœ… **Step 5**: Deploy Node Exporter as a DaemonSet  
âœ… **Step 6**: Create a ServiceMonitor to let Prometheus scrape Node Exporter  
âœ… **Step 7**: Deploy Grafana for visualization

---

## Final Thoughts

- Prometheus Operator creates a **Prometheus StatefulSet** (for managing Prometheus instances).
- Prometheus itself scrapes **Node Exporter DaemonSet** (which collects per-node metrics).
- **ServiceMonitor** links Node Exporter to Prometheus, telling it what to scrape.
- **Grafana** visualizes the data.

---

### Would you like a step-by-step Helm approach as well? ðŸš€

---


