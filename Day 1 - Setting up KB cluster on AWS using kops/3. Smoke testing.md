
kubectl cluster-info

```
kubectl cluster-info
Kubernetes control plane is running at https://api.cloudadhanik.xyz
CoreDNS is running at https://api.cloudadhanik.xyz/api/v1/namespaces/kube-system/services/kube-dns:dns/proxy

To further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.

```
kubectl get ns  
```
kubectl get ns
NAME              STATUS   AGE
default           Active   21m
kube-node-lease   Active   21m
kube-public       Active   21m
kube-system       Active   21m
```

kubectl get pods -n kube-system
kubectl get pods -o wide -n  kube-system | grep api
kubectl get pods  -o wide -n kube-system | grep etcd
kubectl get pods  -o wide -n kube-system | grep scheduler
kubectl get pods  -o wide -n kube-system | grep controller

We can see that they are all running on same IP
```
ubuntu@ip-172-31-81-228:~$ kubectl get pods -o wide -n  kube-system | grep api
kube-apiserver-i-00320e17ca4311e41              2/2     Running   2 (27m ago)   25m   172.20.59.112    i-00320e17ca4311e41   <none>           <none>
ubuntu@ip-172-31-81-228:~$ kubectl get pods  -o wide -n kube-system | grep etcd
etcd-manager-events-i-00320e17ca4311e41         1/1     Running   0             26m   172.20.59.112    i-00320e17ca4311e41   <none>           <none>
etcd-manager-main-i-00320e17ca4311e41           1/1     Running   0             26m   172.20.59.112    i-00320e17ca4311e41   <none>           <none>
ubuntu@ip-172-31-81-228:~$ kubectl get pods  -o wide -n kube-system | grep scheduler
kube-scheduler-i-00320e17ca4311e41              1/1     Running   0             25m   172.20.59.112    i-00320e17ca4311e41   <none>           <none>
ubuntu@ip-172-31-81-228:~$ kubectl get pods  -o wide -n kube-system | grep controller
aws-cloud-controller-manager-vpdls              1/1     Running   0             26m   172.20.59.112    i-00320e17ca4311e41   <none>           <none>
dns-controller-544684c6d-sj97l                  1/1     Running   0             26m   172.20.59.112    i-00320e17ca4311e41   <none>           <none>
ebs-csi-controller-6f4db4fcd-gn92b              5/5     Running   0             26m   172.20.59.112    i-00320e17ca4311e41   <none>           <none>
kops-controller-kvhwm                           1/1     Running   0             26m   172.20.59.112    i-00320e17ca4311e41   <none>           <none>
kube-controller-manager-i-00320e17ca4311e41     1/1     Running   3 (27m ago)   26m   172.20.59.112    i-00320e17ca4311e41   <none>           <none>
ubuntu@ip-172-31-81-228:~$ 

```

# How to deploy resources on KB cluster?

There are 2 ways -

1. Imperative 

```kubectl run testpod --image nginx:latest --dry-run -o yaml```
This generates yaml file for you. You can vi testpod.yaml, paste the yaml from above in this file, and run `kubectl apply -f testpod.yaml`, and your pod will be created

`kubectl get pods`
NAME      READY   STATUS    RESTARTS   AGE
testpod   1/1     Running   0          8s


2. Declarative

We write the yaml file for ourself.


## Note

Dont delete cluster from AWS console directly, as auto healing will bring another up. Hence use 

`kops delete -f cluster.yaml  --yes`