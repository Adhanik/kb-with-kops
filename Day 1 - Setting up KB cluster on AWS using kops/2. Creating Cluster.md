

`kubectl config view` - Initially, since our cluster is not created yet, it should be empty. `kubectl` uses a configuration file (usually located at `~/.kube/config`) to find and connect to the Kubernetes API server.

```
apiVersion: v1
clusters: null
contexts: null
current-context: ""
kind: Config
preferences: {}
users: null

```

# Create a Cluster using Kops and generate a cluster file and save it carefully and do neccessary changes

The below command will generate a template yaml file for you which helps us with creating cluster.

```

kops create cluster --name=cloudadhanik.xyz \
--state=s3://cloudadhanik.xyz --zones=us-east-1a,us-east-1b \
--node-count=2 --control-plane-count=1 --node-size=t3.medium --control-plane-size=t3.medium \
--control-plane-zones=us-east-1a --control-plane-volume-size 10 --node-volume-size 10 \
--ssh-public-key ~/.ssh/id_ed25519.pub \
--dns-zone=cloudadhanik.xyz --dry-run --output yaml

```

We will copy the yaml geneerated, and vi cluster.yaml, and paste it in the content here.

# One done run below commands to create the cluster 

1. `kops create -f cluster.yaml`

  Created cluster/cloudadhanik.xyz
  Created instancegroup/control-plane-us-east-1a
  Created instancegroup/nodes-us-east-1a
  Created instancegroup/nodes-us-east-1b
  Added ssh credential

To deploy these resources, run: kops update cluster --name cloudadhanik.xyz --yes

2. `kops update cluster --name cloudadhanik.xyz --yes --admin`

Cluster is starting.  It should be ready in a few minutes.

Suggestions:
 * validate cluster: kops validate cluster --wait 10m
 * list nodes: kubectl get nodes --show-labels
 * ssh to a control-plane node: ssh -i ~/.ssh/id_rsa ubuntu@api.cloudadhanik.xyz
 * the ubuntu user is specific to Ubuntu. If not using Ubuntu please use the appropriate user based on your OS.
 * read about installing addons at: https://kops.sigs.k8s.io/addons.

2.a kops export kubecfg --name=cloudadhanik.xyz (# Not needed, this happens because the --admin flag in the kops update cluster command adds an admin user and directly modifies the kubeconfig file, providing immediate access to the cluster.)


3. kops validate cluster --wait 10m

```
Validating cluster cloudadhanik.xyz

INSTANCE GROUPS
NAME				ROLE		MACHINETYPE	MIN	MAX	SUBNETS
control-plane-us-east-1a	ControlPlane	t3.medium	1	1	us-east-1a
nodes-us-east-1a		Node		t3.medium	1	1	us-east-1a
nodes-us-east-1b		Node		t3.medium	1	1	us-east-1b

NODE STATUS
NAME			ROLE		READY
i-00320e17ca4311e41	control-plane	True
i-0a7078571c7e3c7d8	node		True
i-0db3e6f8d904c0e44	node		True

```
4. kubectl get nodes

    ```
    kubectl get nodes
    NAME                  STATUS   ROLES           AGE     VERSION
    i-00320e17ca4311e41   Ready    control-plane   8m35s   v1.28.11
    i-0a7078571c7e3c7d8   Ready    node            5m19s   v1.28.11
    i-0db3e6f8d904c0e44   Ready    node            5m15s   v1.28.11

    ```
5. kops delete -f cluster.yml  --yes


### SUMMARY

### Breakdown of the Output and Cluster Configuration

1. **Cluster Configuration (from the `cluster.yaml`)**:
   You created your `cluster.yaml` by running:
   ```bash
   kops create cluster --name=cloudadhanik.xyz --state=s3://cloudadhanik.xyz --zones=us-east-1a,us-east-1b --node-count=2 --control-plane-count=1 --node-size=t3.medium --control-plane-size=t3.medium --control-plane-zones=us-east-1a --control-plane-volume-size 10 --node-volume-size 10 --ssh-public-key ~/.ssh/id_ed25519.pub --dns-zone=cloudadhanik.xyz --dry-run --output yaml
   ```
   This command generated a YAML file (`cluster.yaml`) that defines the configuration of your cluster, including:
   - **Name**: `cloudadhanik.xyz`
   - **State**: The cluster's state is stored in an S3 bucket (`s3://cloudadhanik.xyz`).
   - **Availability Zones**: `us-east-1a`, `us-east-1b`
   - **Node Count**: 2 worker nodes
   - **Control Plane Count**: 1 control plane node
   - **Machine Types**: `t3.medium` instances for both the control plane and worker nodes.
   - **Volume Sizes**: 10 GB EBS volumes for both control plane and worker nodes.
   - **SSH Key**: You provided your public SSH key (`~/.ssh/id_ed25519.pub`).
   - **DNS Zone**: Managed by Route 53 for the domain `cloudadhanik.xyz`.

### Cluster Validation Output
After applying this configuration and running `kops update cluster`, you ran `kops validate cluster`, which checks the status of your cluster. Here's the breakdown of the validation output:

---

### 1. **INSTANCE GROUPS**:
   This section describes the instance groups (sets of nodes) that were created for your cluster. These are divided into the control plane and worker nodes.

   - **Name**: 
     - `control-plane-us-east-1a`: The instance group for the control plane node, located in the `us-east-1a` Availability Zone.
     - `nodes-us-east-1a` and `nodes-us-east-1b`: The instance groups for the worker nodes, one in each zone (`us-east-1a` and `us-east-1b`).
   
   - **Role**:
     - `ControlPlane`: The control plane node, which manages the Kubernetes cluster and schedules workloads.
     - `Node`: The worker nodes that actually run the application workloads.
   
   - **Machine Type**: All instance groups are running on `t3.medium` EC2 instances.

   - **Min/Max**:
     - For the control plane, you have 1 node (min = 1, max = 1).
     - For the worker nodes, you have 1 node in each zone (`us-east-1a` and `us-east-1b`), with both the min and max set to 1.

   - **Subnets**: These indicate the Availability Zones (AZs) where the nodes are deployed:
     - Control plane node: `us-east-1a`
     - Worker nodes: `us-east-1a` and `us-east-1b`

---

### 2. **NODE STATUS**:
   This section shows the status of the nodes in your cluster. All nodes should have a status of `READY`, meaning they are successfully registered with the control plane and can run workloads.

   - **Name**: This is the EC2 instance ID for each node.
     - `i-00320e17ca4311e41`: The control plane node.
     - `i-0a7078571c7e3c7d8`: The worker node in `us-east-1a`.
     - `i-0db3e6f8d904c0e44`: The worker node in `us-east-1b`.
   
   - **Role**: 
     - `control-plane`: The control plane node, responsible for managing the cluster.
     - `node`: The worker nodes that run the application workloads.
   
   - **READY**: All nodes are marked as `True`, meaning they are ready and running successfully.

---

### Summary of What Happened:
- **Instance Groups**: You created 3 instance groups â€” one for the control plane and two for the worker nodes, each in a different availability zone.
- **Node Status**: The cluster is healthy, with the control plane and both worker nodes running successfully (`READY` is `True`).

### Additional Steps:
1. **Check Nodes in Kubernetes**:
   You can further confirm the status of your nodes with `kubectl`:
   ```bash
   kubectl get nodes
   ```

2. **Deploy Applications**:
   Now that your cluster is up and running, you can start deploying workloads using `kubectl`.

3. **Scale the Cluster** (if needed):
   You can scale the number of nodes or the instance types by modifying the instance group definitions in Kops.

This confirms that your Kubernetes cluster is successfully created, and you can proceed with using it! Let me know if you need further assistance.