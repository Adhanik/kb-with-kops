

### 1. Copy Keys for Cluster Creation

Copy the keys and certificates to the appropriate directories (`.ssh/` and `private key`).

We already have our private key id_ed25519 inside .ssh folder already which we did using `ssh-keygen`

```

ubuntu@ip-172-31-25-177:~/.ssh$ ls -ltr
total 12
-rw------- 1 ubuntu ubuntu 391 Nov  2 11:05 authorized_keys
-rw-r--r-- 1 ubuntu ubuntu 105 Nov  2 11:11 id_ed25519.pub
-rw------- 1 ubuntu ubuntu 419 Nov  2 11:11 id_ed25519
ubuntu@ip-172-31-25-177:~/.ssh$ 
```

### 2. Create Namespaces

Create two namespaces: `development` and `production`.

```
ubuntu@ip-172-31-25-177:~$ kubectl create ns development
namespace/development created
ubuntu@ip-172-31-25-177:~$ kubectl create ns production
namespace/production created
ubuntu@ip-172-31-25-177:~$ kubectl get ns
NAME              STATUS   AGE
default           Active   35m
development       Active   27s
kube-node-lease   Active   35m
kube-public       Active   35m
kube-system       Active   35m
production        Active   21s
ubuntu@ip-172-31-25-177:~$ 

```

### 3. Copy CA Certificates

Copy the following files from the master to the management server:
- `ca.crt`
- `ca.key`

So for getting the crt keys from our master node, we will login to our master node, and go to this path 

```
ubuntu@i-06cd00b80475fb4f5:~$ cd /etc/kubernetes/kops-controller/
ubuntu@i-06cd00b80475fb4f5:/etc/kubernetes/kops-controller$ ls -al
total 28
drwxr-xr-x 2 root            root 4096 Nov  2 11:18 .
drwxr-xr-x 6 root            root 4096 Nov  2 11:18 ..
-rw------- 1 kops-controller root  410 Nov  2 11:18 keypair-ids.yaml
-rw-r--r-- 1 kops-controller root 1200 Nov  2 11:18 kops-controller.crt
-rw------- 1 kops-controller root 1679 Nov  2 11:18 kops-controller.key
-rw------- 1 kops-controller root 1090 Nov  2 11:18 kubernetes-ca.crt
-rw------- 1 kops-controller root 1675 Nov  2 11:18 kubernetes-ca.key
ubuntu@i-06cd00b80475fb4f5:/etc/kubernetes/kops-controller$ 
```
We will do sudo cat kubernetes-ca.crt and cat kubernetes-ca.key , 

- Go to management server, and create a new dir /tmp, and paste the content of our key inside ca.crt. Similarly we can do for ca.key

So now we have both our ca.crt and ca.key present in same dir

```
-rw-rw-r--  1 ubuntu ubuntu 1090 Nov  2 12:18 ca.crt
-rw-rw-r--  1 ubuntu ubuntu 1675 Nov  2 12:17 ca.key
```

### 4. Create Users on Mgmt server in /tmp dir

The ca.crt and ca.key are being used here for users cert generation which we copied from master node.

#### User 1: `user1`

Generate the key and certificate:

```bash
openssl genrsa -out user1.key 2048
openssl req -new -key user1.key -out user1.csr -subj "/CN=user1/O=development"
openssl x509 -req -in user1.csr -CA ca.crt -CAkey ca.key -CAcreateserial -out user1.crt -days 365
```

The first command creates a - user1.key
We are creating the above user for development ns in second command - user1.csr
In third command, we are creating user using ca.crt and ca.key which we pasted in our mgmt server and user will have 365 days permission. This will generate user1.crt

```
Certificate request self-signature ok
subject=CN = user1, O = development
```

#### User 2: `user2`

Similarly we will generate for user2 in production ns

```bash
openssl genrsa -out user2.key 2048
openssl req -new -key user2.key -out user2.csr -subj "/CN=user2/O=production"
openssl x509 -req -in user2.csr -CA ca.crt -CAkey ca.key -CAcreateserial -out user2.crt -days 365
```

Certificate request self-signature ok
subject=CN = user2, O = production

#### Output

```
-rw-rw-r-- 1 ubuntu ubuntu 1675 Nov  2 12:17 ca.key
-rw-rw-r-- 1 ubuntu ubuntu 1090 Nov  2 12:18 ca.crt
-rw------- 1 ubuntu ubuntu 1704 Nov  2 12:24 user1.key
-rw-rw-r-- 1 ubuntu ubuntu  915 Nov  2 12:25 user1.csr
-rw-rw-r-- 1 ubuntu ubuntu 1021 Nov  2 12:25 user1.crt
-rw------- 1 ubuntu ubuntu 1704 Nov  2 12:39 user2.key
-rw-rw-r-- 1 ubuntu ubuntu  915 Nov  2 12:39 user2.csr
-rw-rw-r-- 1 ubuntu ubuntu   41 Nov  2 12:40 ca.srl
-rw-rw-r-- 1 ubuntu ubuntu 1021 Nov  2 12:40 user2.crt
```

### 5. Copy Certificates and Keys

- We will create these keys in root dir in master node. Copy all `.crt` and `.key` files to the master root location safely. For the instruction, the root dir was /root, Since we are in ubuntu, we have root dir as /home/ubuntu

- For user1 we have 3 keys, crt, csr(ignore), and key . We will copy these keys to master location.

- These are copied because this will be used in kube config file in client-certificate and client-key arguments.

`cd`

`vi user1.cert`
`vi user1.key`

`vi user2.cert`
`vi user2.key`

### 6. Create Config Files for Users on master

We need to create config file for both the users. We can copy the kube config file from our mgmt server

```
 cd .kube
ubuntu@ip-172-31-25-177:~/.kube$ ls -al
total 20
drwxr-xr-x 3 ubuntu ubuntu 4096 Nov  2 11:38 .
drwxr-x--- 5 ubuntu ubuntu 4096 Nov  2 12:18 ..
drwxr-x--- 4 ubuntu ubuntu 4096 Nov  2 11:38 cache
-rw------- 1 ubuntu ubuntu 5688 Nov  2 11:17 config
ubuntu@ip-172-31-25-177:~/.kube$ cat config 

```
Copy the content of this config file for reference.

Note - A file that is used to configure access to clusters is called a kubeconfig file. It is generic way of referring to configuration files. It does not mean that there is a file named kubeconfig.


## Making changes to content of kube config file

Following has to be done on On master node

- We can see that we have client-certificate-data and client-key-data fields in kube config file. We will give location of our crt and key files here which are stored on master, and change name to client-certificate and client-key

`client-certificate-data: /root/user1.cert
client-key-data: /root/user1.key`

Its important to make this change -

`client-certificate: /root/user1.cert
client-key: /root/user1.key`

- We have updated name,user and current context to user1 and path to keys along with . For user1, ns will be development, for user2 ns will be production.

- We will paste both config file in MASTER NODE vi USER1-CONFIG, and run `export KUBECONFIG=/root/USER1-CONFIG`

- Now if you do kubectl get pods, you will get a forbidden error. This is because user1 does not have access to resource of cluster. for this we need to crate role and role bindings so user1 is able to access the resources.

- remove preferences:{} from your yaml

- After saving the above file, run `export KUBECONFIG=/home/ubuntu/USER1-CONFIG` 

Once we do - `kubectl get pods`, we get

```
kubectl get pods
Error from server (Forbidden): pods is forbidden: User "user1" cannot list resource "pods" in API group "" in the namespace "development"
ubuntu@i-0e16a40ec029a68a8:~$ 
```

- Do same for user 2, vi USER2-CONFIG, and run `export KUBECONFIG=/root/USER2-CONFIG`

- We get same error for user2 as well.

```
kubectl get pods
Error from server (Forbidden): pods is forbidden: User "user2" cannot list resource "pods" in API group "" in the namespace "production"
ubuntu@i-0e16a40ec029a68a8:~$ 
```
# Summary - What happended till now

### 1. Setting Up a Kubernetes Cluster with Kops
When you run `kops create -f cluster.yaml`, you’re telling Kops to set up a Kubernetes cluster based on the specifications defined in `cluster.yaml`. This cluster consists of:
   - A **management server** where you’re running administrative commands.
   - A **control plane** or **master node** that manages the cluster and serves as the central point for API requests, scheduling, etc.
   - **Worker nodes** where your applications (pods) will run.

### 2. Why are Certificates Needed for User Access?
In Kubernetes, **authentication** and **authorization** are two distinct steps:
   - **Authentication** is about verifying the identity of users or services accessing the cluster.
   - **Authorization** determines what these authenticated users are allowed to do.

Kubernetes supports several authentication methods, including **client certificates**, tokens, and OpenID Connect. Here, you’re using **client certificates** to authenticate users.

### 3. SSL Commands to Generate Certificates for Users
Let's dive into the commands you ran to create certificates:

```bash
openssl genrsa -out user1.key 2048
```
- **`openssl genrsa`** generates a private key (`user1.key`) for the user `user1`.
- The **private key** is unique to the user and should be kept secret.

```bash
openssl req -new -key user1.key -out user1.csr -subj "/CN=user1/O=development"
```
- **`openssl req -new`** creates a **Certificate Signing Request (CSR)** for `user1`.
- The CSR includes the **Common Name (CN)**, which specifies the username (`user1`), and the **Organization (O)**, which is often used for grouping (in this case, `development`).
- The **CSR** is like a request for identity verification.

```bash
openssl x509 -req -in user1.csr -CA ca.crt -CAkey ca.key -CAcreateserial -out user1.crt -days 365
```
- **`openssl x509 -req`** signs the CSR using the **Cluster CA (Certificate Authority)** (`ca.crt` and `ca.key`), generating a certificate for the user (`user1.crt`).
- By using the **CA's private key** (`ca.key`), the master node (control plane) certifies that `user1.crt` is valid and trusted by the cluster.

In summary, these commands create a **signed certificate** (`user1.crt`) for the user, allowing them to prove their identity to Kubernetes when making API requests.

### 4. Why Transfer CA Certificates from Master to Management Server?
The **master node** (control plane) has the **CA certificate (`ca.crt`) and private key (`ca.key`)** that Kubernetes uses to sign and validate certificates. You transfer these to the management server temporarily to:
   - Sign user certificates (like `user1.crt`) directly from the management server without needing to generate certificates on the master node.
   - Maintain a separation of duties, as the management server is used for administration tasks, whereas the master node handles core cluster responsibilities.

After generating user certificates, you can transfer them back to the master node, where the Kubernetes API can reference them for user authentication.

### 5. Kubernetes Config Files and RBAC Roles
Now, let’s clarify the steps involving **Kubernetes config files and RBAC roles**:

- **Kubeconfig**: After creating the certificate for `user1`, you need a **Kubeconfig** file to define how `kubectl` (the Kubernetes CLI) can interact with the cluster. This file specifies the **cluster server URL**, **certificate authority**, **user credentials**, and **current context**.

   ```yaml
   apiVersion: v1
   kind: Config
   users:
     - name: user1
       user:
         client-certificate-data: (Base64 user1.crt)
         client-key-data: (Base64 user1.key)
   ```
   - This file allows `user1` to authenticate against the API server using their client certificate.

- **RBAC (Role-Based Access Control)**: 
   - RBAC is Kubernetes’ way to control what authenticated users can do within the cluster. It’s often set up from the **management server** because that’s where you run `kubectl`.
   - By creating **roles** and **role bindings** in namespaces like `dev` and `prod`, you specify what resources users like `user1` can access and modify.

For example:
   ```yaml
   kind: Role
   apiVersion: rbac.authorization.k8s.io/v1
   metadata:
     namespace: dev
     name: developer
   rules:
   - apiGroups: [""]
     resources: ["pods"]
     verbs: ["get", "list", "create"]
   ```

   Here, the `developer` role in the `dev` namespace allows `user1` to list, get, and create pods. You would then bind this role to `user1` using a **RoleBinding**:
   ```yaml
   kind: RoleBinding
   apiVersion: rbac.authorization.k8s.io/v1
   metadata:
     name: developer-binding
     namespace: dev
   subjects:
   - kind: User
     name: user1
   roleRef:
     kind: Role
     name: developer
     apiGroup: rbac.authorization.k8s.io
   ```

### 6. Why Can’t We Manage Everything from the Management Server?
While the management server can handle creating users and RBAC roles, Kubernetes enforces some tasks at the **control plane** level, such as verifying client certificates. Thus, some files (like the user’s Kubeconfig) need to be transferred back to the master node where the Kubernetes API server can authenticate requests.

### In Summary:
1. **Generate certificates** for users on the management server.
2. **Transfer user certificates** to the master node for Kubernetes API authentication.
3. **Configure Kubeconfig** files on the management server for user authentication.
4. **Set up RBAC roles and bindings** on the management server, defining permissions within namespaces.
5. **Authenticate via Kubeconfig**: Once configured, `user1` can use the Kubeconfig to interact with the cluster, and RBAC policies will restrict access as defined.

Using this approach allows you to set up **secure and controlled access** to the cluster while separating certificate generation from the master node’s core responsibilities. Let me know if there’s anything specific you’d like to go into further detail on!


Contd in next -->